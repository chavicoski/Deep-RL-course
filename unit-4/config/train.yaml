defaults:
  - _self_
  - train_env: pixelcopter_ple_v0
  - eval_env: pixelcopter_ple_v0

experiment_name: "Reinforce_Pixelcopter-PLE-v0"

# Computing device
device: "gpu" # Options: "gpu" or "cpu"

policy:
  _target_: lib.reinforce.Policy
  h_size: 64  # Hidden size of the DNN
  n_layers: 3  # Should be at least 2
  # `state_size` and `action_size` are obtained during runtime from the environment

optimizer:
  _target_: torch.optim.Adam
  lr: 0.0001

train_hparams:
  n_training_episodes: 100000  # Total number of episodes to train the agent
  max_steps: 10000  # Max steps per episode
  gamma: 0.99  # Exploration/exploitation factor
  print_every: 100  # Number of episodes between each training status print

eval_hparams:
  n_eval_episodes: 10
  max_steps: 10000

# File to store the trained model parameters
model_path: "model.pt"

# Path to store all the config params into a JSON file
hparams_path: "hyperparameters.json"

# Replay video config
video_path: "replay.mp4"
video_fps: 30

hydra:
  run:
    # The experiment outputs will be stored in this folder
    dir: runs/train/${now:%Y-%m-%d_%H-%M-%S}
  job:
    chdir: True
